<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>How CRVE works?</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        background-color: #232629;
        color: #7a7c7d;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #7a7c7d;  padding-left: 4px; }
    div.sourceCode
      { color: #cfcfc2; background-color: #232629; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #cfcfc2; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #3f8058; } /* Annotation */
    code span.at { color: #2980b9; } /* Attribute */
    code span.bn { color: #f67400; } /* BaseN */
    code span.bu { color: #7f8c8d; } /* BuiltIn */
    code span.cf { color: #fdbc4b; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #3daee9; } /* Char */
    code span.cn { color: #27aeae; font-weight: bold; } /* Constant */
    code span.co { color: #7a7c7d; } /* Comment */
    code span.cv { color: #7f8c8d; } /* CommentVar */
    code span.do { color: #a43340; } /* Documentation */
    code span.dt { color: #2980b9; } /* DataType */
    code span.dv { color: #f67400; } /* DecVal */
    code span.er { color: #da4453; text-decoration: underline; } /* Error */
    code span.ex { color: #0099ff; font-weight: bold; } /* Extension */
    code span.fl { color: #f67400; } /* Float */
    code span.fu { color: #8e44ad; } /* Function */
    code span.im { color: #27ae60; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #cfcfc2; font-weight: bold; } /* Keyword */
    code span.op { color: #cfcfc2; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #27ae60; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #3daee9; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #f44f4f; } /* String */
    code span.va { color: #27aeae; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h3 id="notation">Notation</h3>
<p>We use the same notation as in <span class="citation"
data-cites="cameron2015cluster">[@cameron2015cluster]</span>: <span
class="math display">\[
\mathbf{X_{ig}}=
\begin{bmatrix}
x_{1ig}, &amp;  x_{2ig}, &amp;  \dots  &amp; x_{kig}
\end{bmatrix}
\]</span> <span class="math display">\[
\mathbf{X_{g}}= \begin{bmatrix}
\mathbf{X_{ig}} \\
\mathbf{X_{jg}} \\  
\dots  \\
\mathbf{X_{N_{g}g}}
\end{bmatrix} =
\begin{bmatrix}
x_{1ig}, &amp;  x_{2ig}, &amp;  \dots  &amp; x_{kig}  \\
x_{1ij}, &amp;  x_{2jg}, &amp;  \dots  &amp; x_{kjg}  \\
\dots \\
x_{1N_{g}g}, &amp;  x_{2N_{g}g}, &amp;  \dots  &amp; x_{kN_{g}g}
\end{bmatrix}
\]</span> <span class="math display">\[
\mathbf{X} =
\begin{bmatrix}
\mathbf{X_{ig}} \\
\mathbf{X_{jg}} \\
\dots \\
\mathbf{X_{N_{g}g}} \\
\\
\mathbf{X_{ih}} \\
\mathbf{X_{jh}} \\
\dots \\
\mathbf{X_{N_{h}h}} \\
\\
\dots \\
\\
\mathbf{X_{iG}} \\
\mathbf{X_{jG}} \\
\dots \\
\mathbf{X_{N_{G}G}}
\end{bmatrix}
\]</span> —</p>
<p>The idea behind CRVE is that clusters are independent one from the
other. Since inference requires independence across a big number of
units, we base it on the number of clusters rather than the number of
individuals. Assuming that <span class="math inline">\(\mathbb{E}\left[
\mathbf{X&#39;_{g}u_{g}} \right]=0\)</span>, <span
class="math inline">\(\hat{\beta}_\text{ols}\)</span> will be a
consistent estimator of the population parameter <span
class="math inline">\(\beta\)</span> if <span class="math inline">\(G
\to \infty\)</span> <span class="citation"
data-cites="woolridge2010">[@woolridge2010]</span>. To understand this
note that computing <span class="math inline">\(\beta\)</span>: <span
class="math display">\[
\left( X&#39;X \right)^{-1}\left( X&#39;y \right)
\]</span> is the same as doing: <span class="math display">\[
\hat{\boldsymbol{\beta}}_\text{ols} = \left(
\sum_{g}^{G}\mathbf{X_{g}}&#39;\mathbf{X_{g}}  \right)^{-1}\left(
\sum_{g}^{G}\mathbf{X_{g}&#39;y_{g}} \right)
\]</span> so <span class="math display">\[
\left( \hat{\boldsymbol{\beta}}_\text{ols} -\boldsymbol{\beta} \right) =
\left(  \frac{1}{G}\sum_{g}^{G}\mathbf{X_{g}}&#39;\mathbf{X_{g}}  \right)^{-1}\left(
\frac{1}{G}\sum_{g}^{G}\mathbf{X_{g}&#39;u_{g}} \right)
\]</span> and applying the Weak Law of Large Numbers <span
class="citation" data-cites="woolridge2003">[@woolridge2003]</span>
<span class="math display">\[
\frac{1}{G}\sum_{g}^{G}\mathbf{X_{g}&#39;u_{g}} \xrightarrow{p}
\mathbb{E}\left[ \mathbf{X&#39;_{g}u_{g}} \right]=0
\]</span> so <span
class="math inline">\(\hat{\boldsymbol{\beta}}_\text{ols}\)</span> will
be consistent. Now let us derive it’s asymptotic distribution: <span
class="math display">\[
\sqrt{ G } \left( \hat{\boldsymbol{\beta}}_\text{ols}
-\boldsymbol{\beta} \right) =
\left(  \frac{1}{G}\sum_{g}^{G}\mathbf{X_{g}}&#39;\mathbf{X_{g}}  \right)^{-1}\left(
\frac{1}{\sqrt{ G }}\sum_{g}^{G}\mathbf{X_{g}&#39;u_{g}} \right)
\]</span> The first term is by assumption <span
class="math inline">\(A^{-1} + o_{p}(1)\)</span>, while the second is
<span class="math inline">\(\overset{a}{\sim} \mathcal{N}\left( 0,
\mathbb{E}[\mathbf{X_{g}&#39;u_{g}u_{g}&#39;X_{g}}] \right)\)</span> So
<span class="math display">\[
\sqrt{ G } \left( \hat{\boldsymbol{\beta}}_\text{ols}
-\boldsymbol{\beta} \right) \xrightarrow{d} \mathcal{N} \left( 0,
A^{-1}\mathbb{E}[\mathbf{X_{g}&#39;u_{g}u_{g}&#39;X_{g}}]A^{-1} \right)
\]</span></p>
<h3 id="formula">Formula</h3>
<p><span class="math display">\[
\hat{V}_\text{clu} \hat{\mathbf{\beta}} = \left( \mathbf{X&#39;X}
\right)^{-1} \mathbf{B} \left(  \mathbf{X&#39;X} \right)^{-1} = \left(
\mathbf{X&#39;X} \right)^{-1}
\left(  \sum_{g=1}^{G}\mathbf{X}_{g}&#39;\hat{\mathbf{u}}_{g}\hat{\mathbf{u}}_{g}&#39;\mathbf{X}_{g
}  \right) \left(  \mathbf{X&#39;X} \right)^{-1}
\]</span> Assuming that we have our data stored in a pandas DataFrame
(called <code>df</code>), we first compute <span
class="math inline">\((\mathbf{X&#39;X})^{-1}\)</span> and initialize
the inner matrix <span class="math inline">\(\mathbf{B}\)</span> to a
matrix full of zeroes with the same dimensions as <span
class="math inline">\(\left( \mathbf{X&#39;X} \right)^{-1}\)</span>
.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.<span class="bu">filter</span>(like<span class="op">=</span><span class="st">&#39;X&#39;</span>).to_numpy()</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>XX_inv <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>B_hat_sum <span class="op">=</span> np.zeros_like(XX_inv)</span></code></pre></div>
<p>The estimated version of <span
class="math inline">\(\mathbf{B}\)</span> would of course be: <span
class="math display">\[
\mathbf{X_{g}&#39;\hat{u}_{g}\hat{u}_{g}&#39;X_{g}}
\]</span> Define: <span class="math display">\[
\mathbf{s_g}\equiv\mathbf{X_{g}}&#39; \mathbf{\hat u_{g}} =
\begin{bmatrix}
X_{1 1 g} &amp; X_{1 2 g} &amp; \cdots &amp; X_{1 N_g g} \\
X_{2 1 g} &amp; X_{2 2 g} &amp; \cdots &amp; X_{2 N_g g} \\
\vdots    &amp; \vdots    &amp; \ddots &amp; \vdots      \\
X_{k 1 g} &amp; X_{k 2 g} &amp; \cdots &amp; X_{k N_g g}
\end{bmatrix}
\begin{bmatrix}
\hat u_{1 g} \\
\hat u_{2 g} \\
\vdots       \\
\hat u_{N_g g}
\end{bmatrix}
=
\begin{bmatrix}
\displaystyle \sum_{i=1}^{N_g} X_{1 i g}\, \hat u_{i g} \\
\displaystyle \sum_{i=1}^{N_g} X_{2 i g}\, \hat u_{i g} \\
\vdots \\
\displaystyle \sum_{i=1}^{N_g} X_{k i g}\, \hat u_{i g}
\end{bmatrix}
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\mathbf{s_g s_g}&#39;
= (X_g&#39; {u}_g)(X_g&#39; {u}_g)&#39;
= X_g&#39; {u}_g {u}_g&#39; X_g
\]</span></p>
<p><span class="math display">\[
\mathbf{s_g s_g}&#39; =
\begin{bmatrix}
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{1 i g} X_{1 j g} \hat
u_{i g} \hat u_{j g} &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{1 i g} X_{2 j g} \hat
u_{i g} \hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{1 i g} X_{k j g} \hat
u_{i g} \hat u_{j g} \\[6pt]
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{2 i g} X_{1 j g} \hat
u_{i g} \hat u_{j g} &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{2 i g} X_{2 j g} \hat
u_{i g} \hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{2 i g} X_{k j g} \hat
u_{i g} \hat u_{j g} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[6pt]
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{k i g} X_{1 j g} \hat
u_{i g} \hat u_{j g} &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{k i g} X_{2 j g} \hat
u_{i g} \hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{i \in g} \sum_{j \in g} X_{k i g} X_{k j g} \hat
u_{i g} \hat u_{j g}
\end{bmatrix}\]</span></p>
<p>We can do this by looping through all clusters:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, df_g <span class="kw">in</span> df.groupby(cluster_col):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>   <span class="co">#df.groupby(col) yields pairs (group_name, group_dataframe)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>   <span class="co">#so df_g is a dataframe with only the observation from a certain cluster</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>   X_g <span class="op">=</span> df_g.<span class="bu">filter</span>(like<span class="op">=</span><span class="st">&#39;X&#39;</span>).to_numpy()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>   u_g <span class="op">=</span> df_g[<span class="st">&#39;uhat&#39;</span>].to_numpy().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>   B_hat_g <span class="op">=</span> X_g.T <span class="op">@</span> (u_g <span class="op">@</span> u_g.T) <span class="op">@</span> X_g</span></code></pre></div>
<blockquote>
<p>[!error] Not sure This matrix refers only to a cluster (The sums are
over <span class="math inline">\(i,j \in g\)</span>), if we were to use
the normal White matrix, we would have the sums over all individuals,
regardless of the clusters.</p>
</blockquote>
<p>Then we sum this over all G clusters:</p>
<p><span class="math display">\[
\mathbf{\hat{B}}=\sum_{g=1}^{G}\mathbf{X}_{g}&#39;\mathbf{\hat
u}_{g}\mathbf{\hat u}_{g}&#39;\mathbf{X}_{g} =
\begin{bmatrix}
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{1 i g}\,X_{1
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{1 i g}\,X_{2
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{1 i g}\,X_{k
j g}\,\hat u_{i g}\,\hat u_{j g} \\[6pt]
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{2 i g}\,X_{1
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{2 i g}\,X_{2
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{2 i g}\,X_{k
j g}\,\hat u_{i g}\,\hat u_{j g} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[6pt]
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{k i g}\,X_{1
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{k i g}\,X_{2
j g}\,\hat u_{i g}\,\hat u_{j g} &amp;
\cdots &amp;
\displaystyle \sum_{g=1}^{G}\sum_{i \in g}\sum_{j \in g} X_{k i g}\,X_{k
j g}\,\hat u_{i g}\,\hat u_{j g}
\end{bmatrix}
\]</span></p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># matrices is a list with all the G s_gs_g matrices</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># stack them into a 3-D array</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>stacked <span class="op">=</span> np.stack(matrices, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sum along the first axis</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.<span class="bu">sum</span>(stacked, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>So what CRVE does is to compute for each cluster the outer product of
<span class="math inline">\(\mathbf{\hat{S}_{g}} =
\mathbf{X_{g}&#39;\hat{u}_{g}}\)</span> with itself, and then average
those matrices across clusters.<br />
After pre and post-multiplication by <span
class="math inline">\(\bigl(\frac{1}{N}\sum_i
\mathbf{x}_i\mathbf{x}_i^\prime\bigr)^{-1}\)</span>, this converges to
the <strong>asymptotic variance-covariance matrix</strong> of the OLS
estimator <span
class="math inline">\(\hat{\boldsymbol{\beta}}_\text{ols}\)</span>.</p>
<hr />
<h2 id="convergence">Convergence</h2>
<p>We want to show that <span class="math display">\[
\frac{1}{G}\sum_{g}
\mathbf{X}_g^{\prime}\,\hat{\mathbf{u}}_g
\hat{\mathbf{u}}_g^{\prime}\mathbf{X}_g
\;\overset{p}{\longrightarrow}\;
\mathbb{E}\!\left[ \mathbf{X}_g^{\prime}\mathbf{u}_g
\mathbf{u}_g^{\prime}\mathbf{X}_g \right]
\]</span> First let’s substitute for <span
class="math inline">\(\mathbf{\hat{u}_{g}}\)</span>:</p>
<p><span class="math display">\[
\frac{1}{G}\sum_{g}
\mathbf{X}_g^{\prime}\bigl(\mathbf{y}_g-\mathbf{X}_g\hat{\boldsymbol{\beta}}\bigr)
\bigl(\mathbf{y}_g-\mathbf{X}_g\hat{\boldsymbol{\beta}}
\bigr)^{\prime}\mathbf{X}_g
\]</span></p>
<p><span class="math display">\[
\frac{1}{G}\sum_{g}
\mathbf{X}_g^{\prime}\bigl(\mathbf{X}_g\boldsymbol{\beta}+\mathbf{u}_g-\mathbf{X}_g\hat{\boldsymbol{\beta}}_g
\bigr)
\bigl(\mathbf{X}_g\boldsymbol{\beta}+\mathbf{u}_g-\mathbf{X}_g\hat{\boldsymbol{\beta}}_g
\bigr)^{\prime}\mathbf{X}_g
\]</span> We then factor terms in order to isolate <span
class="math inline">\((\beta - \hat{\beta})\)</span></p>
<p><span class="math display">\[
\frac{1}{G}\sum_{g}
\mathbf{X}_g^{\prime}\Bigl[\mathbf{X}_g\bigl(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_g
\bigr)+\mathbf{u}_g \Bigr]
\Bigl[\mathbf{X}_g\bigl(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_g
\bigr)+\mathbf{u}_g \Bigr]^{\prime}\mathbf{X}_g
\]</span> As <span class="math inline">\(\hat{\beta} \overset{p}{\to}
\beta\)</span>, the extra terms vanish, leaving only <span
class="math inline">\(\mathbf{u}_g\)</span>. <span
class="math display">\[
\xrightarrow{p}
\frac{1}{G}\sum_{g}
\mathbf{X}_g^{\prime}\mathbf{u}_g \mathbf{u}_g^{\prime}\mathbf{X}_g
\;\overset{p}{\longrightarrow}\;
\mathbb{E}\!\left[ \mathbf{X}_g^{\prime}\mathbf{u}_g
\mathbf{u}_g^{\prime}\mathbf{X}_g \right]
\]</span> &gt;[!question] &gt;- In which way are we taking into account
cluster correlation? show it better &gt;-</p>
<p>#econometrics</p>
</body>
</html>
